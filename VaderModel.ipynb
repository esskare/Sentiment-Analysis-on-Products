{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\n",
      "(413840, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re  # Import re for regex-based tokenization\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# # Define the custom tokenization function\n",
    "# def simple_tokenize(text):\n",
    "#     tokens = text.split()  # Basic tokenization by splitting on whitespace\n",
    "#     return tokens\n",
    "\n",
    "# Reading data \n",
    "df = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
    "\n",
    "# Show the specified index on the 'Reviews' row\n",
    "print(df['Reviews'].values[0])  \n",
    "\n",
    "# Print the shape of the dataset.\n",
    "print(df.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df = df.head(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUICK EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick EDA: Plot the count of reviews by stars\n",
    "ax = df['Rating'].value_counts().sort_index().plot(\n",
    "    kind='bar', \n",
    "    title='Count of Reviews by stars',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "ax.set_xlabel('Review Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC NLTK TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fiance', 'had', 'this', 'phone', 'previously', ',', 'but', 'caused', 'many', 'problems', '.', 'So', ',', 'of', 'course', ',', 'we', 'decided', 'to', 'browse', 'amazon', 'for', 'a', 'replacement', 'til', \"'\", 'our', 'contract', 'is', 'up', '!', '&', 'so', 'far', 'so', 'good', '!']\n",
      "(S\n",
      "  My/PRP$\n",
      "  fiance/NN\n",
      "  had/VBD\n",
      "  this/DT\n",
      "  phone/NN\n",
      "  previously/RB\n",
      "  ,/,\n",
      "  but/CC\n",
      "  caused/VBD\n",
      "  many/JJ\n",
      "  problems/NNS\n",
      "  ./.\n",
      "  So/RB\n",
      "  ,/,\n",
      "  of/IN\n",
      "  course/NN\n",
      "  ,/,\n",
      "  we/PRP\n",
      "  decided/VBD\n",
      "  to/TO\n",
      "  browse/VB\n",
      "  amazon/NN\n",
      "  for/IN\n",
      "  a/DT\n",
      "  replacement/NN\n",
      "  til/NN\n",
      "  '/''\n",
      "  our/PRP$\n",
      "  contract/NN\n",
      "  is/VBZ\n",
      "  up/RP\n",
      "  !/.\n",
      "  &/CC\n",
      "  so/RB\n",
      "  far/RB\n",
      "  so/RB\n",
      "  good/JJ\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "# Basic NLTK Tokenization\n",
    "example = df['Reviews'][10]\n",
    "tokens = nltk.word_tokenize(example)  # Tokenize the 'example' variable\n",
    "print(tokens)\n",
    "tokens[:10] #first 10 words\n",
    "tagged= nltk.pos_tag(tokens)\n",
    "tagged[:10]\n",
    "#putting them into entities\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADERS MODEL\n",
    "- Uses a \"cluster of words\" approach\n",
    "- Stop words are removed e.g. and, or\n",
    "- Each word is combined to a total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487c2ab64951407482638efeecf9fc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Ensure 'Reviews' column is string type and handle missing values\n",
    "df['Reviews'] = df['Reviews'].astype(str)  # Convert all entries to string\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "# Run polarity score on the entire dataset\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['Reviews']\n",
    "    name = row['Brand Name']\n",
    "    \n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        polarity_scores = sia.polarity_scores(text)\n",
    "        polarity_scores['Brand Name'] = name  # Add 'Brand Name' to the results\n",
    "        results.append(polarity_scores)\n",
    "    else:\n",
    "        results.append({\"Brand Name\": name, \"error\": \"Invalid text\"})\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "vaders = pd.DataFrame(results)\n",
    "\n",
    "# Merge the results with the original DataFrame\n",
    "vaders = vaders.merge(df, how='left', on='Brand Name')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound Brand Name  \\\n",
       "0  0.015  0.796  0.189    0.8783    Samsung   \n",
       "1  0.015  0.796  0.189    0.8783    Samsung   \n",
       "2  0.015  0.796  0.189    0.8783    Samsung   \n",
       "3  0.015  0.796  0.189    0.8783    Samsung   \n",
       "4  0.015  0.796  0.189    0.8783    Samsung   \n",
       "\n",
       "                                        Product Name   Price  Rating  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
       "\n",
       "                                             Reviews  Review Votes  \n",
       "0  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2                                       Very pleased           0.0  \n",
       "3  It works good but it goes slow sometimes but i...           0.0  \n",
       "4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No we have sentiment score and metadata\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT VADERS RESULTS\n",
    "- bar plot of the compound of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(data = vaders, x = 'Rating', y='compound')\n",
    "ax.set_title('Compound Score by Amazon Star Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS, Neg, Neu Scores of the star Ratings\n",
    "\n",
    "Observation and Assumptions respectively\n",
    "- The Plots look similar to one another.\n",
    "\n",
    "- Vader tends to pick up on a lot of cluster words and may not be finely tuned to nuances\n",
    "- There are many nuances of specific texts leading to similar sentiment scores across different ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "sns.barplot(data = vaders, x = 'Rating', y='neg', ax=axs[0])\n",
    "sns.barplot(data = vaders, x = 'Rating', y='neu', ax=axs[1])\n",
    "sns.barplot(data = vaders, x = 'Rating', y='pos', ax=axs[2])\n",
    "axs[0].set_title('Negative')\n",
    "axs[1].set_title('Neutral')\n",
    "axs[2].set_title('Positive')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROBERTA PRETRAINED MODEL\n",
    "- Uses a model trained of a large corpus of data\n",
    "- Transformer model accounts for the words but also the context related to other words\n",
    "- Applying the trained weights to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaForSequenceClassification\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Specific model trained\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1594\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraceback\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx_traceback\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fun\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_map\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m capture_logs, LoggingTensorMode\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\functional_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionalTensor\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sparse_any\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m definitely_true, sym_eq\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     is_traceable_wrapper_subclass,\n\u001b[0;32m     23\u001b[0m     transform_subclass,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeGuard, Source, TracingContext\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     FloorDiv, Mod, PythonMod, IsNonOverlappingAndDenseIndicator, CleanDiv, FloorToInt, CeilToInt\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_solve\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalue_ranges\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bound_sympy, SymPyValueRangeAnalysis, ValueRanges, ValueRangeError\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFloorDiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModularIndexing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPowByNatural\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\__init__.py:74\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[0;32m     68\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[0;32m     69\u001b[0m         true, false, satisfiable)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[0;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[0;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[0;32m     77\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[0;32m     78\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[0;32m     79\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[0;32m     80\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, all_roots,\n\u001b[0;32m     81\u001b[0m         real_roots, nroots, ground_roots, nth_power_roots_poly, cancel,\n\u001b[0;32m     82\u001b[0m         reduced, groebner, is_zero_dimensional, GroebnerBasis, poly,\n\u001b[0;32m     83\u001b[0m         symmetrize, horner, interpolate, rational_interpolate, viete, together,\n\u001b[0;32m     84\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[0;32m     85\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[0;32m     86\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[0;32m     87\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[0;32m     88\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[0;32m     89\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[0;32m     90\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[0;32m     91\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[0;32m     92\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[0;32m     93\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[0;32m     94\u001b[0m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[0;32m     95\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[0;32m     96\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[0;32m     97\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[0;32m     98\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[0;32m     99\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[0;32m    100\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[0;32m    101\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[0;32m    102\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[0;32m    103\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[0;32m    104\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[0;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[0;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[0;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[0;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[0;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[0;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[0;32m    136\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\polys\\__init__.py:124\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morthopolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (jacobi_poly, chebyshevt_poly, chebyshevu_poly,\n\u001b[0;32m    119\u001b[0m         hermite_poly, hermite_prob_poly, legendre_poly, laguerre_poly)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mappellseqs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (bernoulli_poly, bernoulli_c_poly, genocchi_poly,\n\u001b[0;32m    122\u001b[0m         euler_poly, andre_poly)\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartfrac\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apart, apart_list, assemble_partfrac_list\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyoptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ring, xring, vring, sring\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\polys\\partfrac.py:13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolytools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel_poly_from_expr\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numbered_symbols, take, xthreaded, public\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;129;43m@xthreaded\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129;43m@public\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mapart\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;43;03m    Compute partial fraction decomposition of a rational function.\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;43;03m    apart_list, assemble_partfrac_list\u001b[39;49;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\utilities\\decorator.py:76\u001b[0m, in \u001b[0;36mxthreaded\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxthreaded\u001b[39m(func):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``func`` to sub--elements of an object, excluding :class:`~.Add`.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This decorator is intended to make it uniformly possible to apply a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreaded_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\utilities\\decorator.py:13\u001b[0m, in \u001b[0;36mthreaded_factory\u001b[1;34m(func, use_add)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A factory for ``threaded`` decorators. \"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sympify\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrices\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatrixBase\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iterable\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreaded_func\u001b[39m(expr, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\matrices\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeError, NonSquareMatrixError\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkind\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatrixKind\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     GramSchmidt, casoratian, diag, eye, hessian, jordan_cell,\n\u001b[0;32m     10\u001b[0m     list2numpy, matrix2numpy, matrix_multiply_elementwise, ones,\n\u001b[0;32m     11\u001b[0m     randMatrix, rot_axis1, rot_axis2, rot_axis3, rot_ccw_axis1,\n\u001b[0;32m     12\u001b[0m     rot_ccw_axis2, rot_ccw_axis3, rot_givens,\n\u001b[0;32m     13\u001b[0m     symarray, wronskian, zeros)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MutableDenseMatrix\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrixbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeferredVector, MatrixBase\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\matrices\\dense.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeError\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cholesky, _LDLdecomposition\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrixbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatrixBase\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MutableRepMatrix, RepMatrix\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolvers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _lower_triangular_solve, _upper_triangular_solve\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\sympy\\matrices\\matrixbase.py:65\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreductions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_echelon, _echelon_form, _rank, _rref\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolvers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     61\u001b[0m     _diagonal_solve, _lower_triangular_solve, _upper_triangular_solve,\n\u001b[0;32m     62\u001b[0m     _cholesky_solve, _LDLsolve, _LUsolve, _QRsolve, _gauss_jordan_solve,\n\u001b[0;32m     63\u001b[0m     _pinv_solve, _cramer_solve, _solve, _solve_least_squares)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     66\u001b[0m     _pinv, _inv_ADJ, _inv_GE, _inv_LU, _inv_CH, _inv_LDL, _inv_QR,\n\u001b[0;32m     67\u001b[0m     _inv, _inv_block)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _columnspace, _nullspace, _rowspace, _orthogonalize\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meigen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     72\u001b[0m     _eigenvals, _eigenvects,\n\u001b[0;32m     73\u001b[0m     _bidiagonalize, _bidiagonal_decomposition,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     _is_negative_definite, _is_negative_semidefinite, _is_indefinite,\n\u001b[0;32m     77\u001b[0m     _jordan_form, _left_eigenvects, _singular_values)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#ROBERTA MODEL\n",
    "import tf_keras as keras\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "#Specific model trained\n",
    "MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sia.polarity_scores(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7586,  0.0125,  0.9647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "{'roberta_neg': 0.11408854, 'roberta_neu': 0.24666648, 'roberta_pos': 0.6392449}\n"
     ]
    }
   ],
   "source": [
    "# encoded text\n",
    "import numpy as np\n",
    "encoded_text = tokenizer(example, return_tensors = \"pt\")\n",
    "output = model(**encoded_text) #tensor with our results\n",
    "print(output) \n",
    "scores = output[0][0].detach().numpy() #convert to numpy array\n",
    "scores = softmax(scores) #apply softmax\n",
    "scores_dict = {\n",
    "    'roberta_neg': scores[0],\n",
    "    'roberta_neu': scores[1],\n",
    "    'roberta_pos': scores[2]\n",
    "}\n",
    "print(scores_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors = \"pt\")\n",
    "    output = model(**encoded_text) #tensor with our results\n",
    "    print(output) \n",
    "    scores = output[0][0].detach().numpy() #convert to numpy array\n",
    "    scores = softmax(scores) #apply softmax\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b321b99f01714911942a6993a141b4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9108, -0.5009,  3.0563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2665, -0.7459,  3.8441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6510,  0.1087,  3.1351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0012, -0.7938,  3.6114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.8940, -0.0059,  1.2029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2062, -0.0542, -2.2693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9189,  0.2051, -1.2075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6769, -0.4487, -2.2836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6853, -0.1345,  2.0856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3583, -0.0849,  1.8801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7586,  0.0125,  0.9647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8129, -0.4314,  2.8464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2735, -0.5568,  3.6031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0224, -0.0515, -2.0543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9416, -0.1758,  2.6924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0538,  0.2407, -0.1688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2343, -0.8584,  3.8957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0642, -0.6929,  3.5008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6654, -0.5435,  2.9444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0886,  0.1306,  1.1260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4495,  0.4439, -0.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2042, -0.3214,  3.1593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5699,  0.3201, -2.0665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3225, -0.8004,  3.8516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3037, -0.3486,  3.2875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2775, -0.9299,  4.2147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0243,  0.0673,  2.2036]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1179, -0.3396,  3.2096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3234,  0.4480,  0.9688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5481,  0.0086, -1.6715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1720,  1.3665,  1.2374]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9388,  0.2534,  2.3417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5922, -0.0982, -2.4969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3694, -0.8725,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7696, -0.0204,  0.9349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5006, -0.8089,  4.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7525, -0.4431,  2.9616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7828,  0.5873,  0.2422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2906, -0.2284,  3.2618]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1200,  0.1858,  2.5783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6328, -0.6300,  4.0745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8010,  0.5964, -1.4016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3826,  0.0188,  0.6666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9682, -0.7445,  3.6183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6168, -0.6128,  3.9381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7645,  0.6223,  2.3120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8445, -0.1709,  3.5810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5647,  0.2155,  2.7969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3291, -0.3228,  3.4832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4142,  0.1316,  1.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1318,  1.0884, -0.9558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5541,  0.0840, -1.8226]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9911,  0.1265,  3.2424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6890, -0.0960,  3.2772]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7989, -0.1146,  3.3425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8098,  0.7268,  1.2729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9396,  0.5231,  1.7694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5281,  0.1547,  2.9814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3030, -0.4073,  3.4522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7442, -0.3284,  3.7037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2995,  0.0425,  1.5016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6427,  1.0813,  1.5591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5953,  0.2559,  1.4737]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2455,  0.6638, -1.9406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4790,  0.0449,  2.8387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.8783,  1.4494, -0.5699]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3224, -0.3982,  3.3853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3003,  0.0551,  1.4866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 2\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3393, -0.4660,  3.5175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1316, -0.1726,  2.7160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5182,  0.0922,  0.5764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2639, -0.5733,  3.4787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6348,  0.3669,  1.3936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5087, -0.7529,  4.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5518, -0.0552, -2.5703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6212, -0.0070,  3.0368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8739, -0.2380,  2.6472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3928,  0.1614,  0.2711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7193, -0.0617,  2.4461]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3473,  0.2499, -0.6188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7455, -0.5295,  4.0284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6471, -0.0918, -1.6617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5837,  0.2653, -0.7167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2410,  0.3416,  1.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6736,  0.2203, -1.9080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8996,  0.1287,  2.3260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5217,  0.9501,  1.0332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2869,  0.3720, -2.6638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3214, -0.6663,  3.7871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6132,  0.0951,  3.0048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4663, -0.2205,  3.1799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0817,  0.0316, -2.2024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4359, -0.1711,  2.0778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5350,  0.1993,  2.7807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1302,  0.4923,  0.8096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6239, -0.4413,  3.7629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6275,  0.3374, -0.9838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4731, -0.1841,  3.2331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4807,  0.2076,  1.4352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9406,  0.2795, -1.3773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1639,  0.3132,  2.2738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4067,  0.0864,  2.8353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1388,  0.1131,  2.5548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6869,  0.3198,  0.4594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2420,  0.8011, -2.1647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7898,  0.1367,  1.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5156, -0.3167,  3.4288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3408,  0.7240, -0.2279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8043,  0.0915,  2.1939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 5\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5719, -0.5103,  3.8171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7299, -0.2749,  3.5566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1746, -0.2693,  1.8140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0848,  0.1347, -1.2147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1171, -0.1491, -2.0626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3035,  0.6018,  1.3568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7769,  1.0268,  0.8429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5841, -0.3018,  3.5620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7812,  0.0395,  1.1541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0664, -0.3517,  3.1525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0158, -0.5613, -2.5145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1822,  0.9579,  1.6937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2211,  0.0330, -1.2818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3370,  0.1579,  2.7191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4366,  0.0415,  2.8584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2904,  0.7204, -0.3723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2907,  0.6577, -0.2187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5005,  1.5223, -1.0166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1302,  0.0962, -1.3451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0349,  0.0813,  1.0727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4005,  0.3125, -0.7398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8972, -0.3140,  2.8109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2350, -0.5875,  3.6318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3844,  0.0552, -2.4408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1952, -0.0846,  1.7677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3327,  0.1422,  2.5745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5528, -0.0681, -2.6333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9507,  0.1970,  0.9200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1192, -0.0259,  1.4657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3909, -0.5227,  3.6433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1914, -0.5352, -2.5305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6043,  0.8007, -1.3935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1554,  0.2634, -2.5007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9507, -0.3976,  2.8746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7327, -0.3890,  3.7022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4115, -0.6234,  3.8958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1627,  0.5740, -0.6397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6159,  0.0251,  1.8720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9270, -0.1841,  1.4465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4283,  0.2519,  2.6460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6288,  0.0938, -0.6371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3521,  1.2858, -1.4397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9475,  0.1096,  1.0521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.2064,  0.3867,  3.3155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6211, -0.2326,  2.3682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3527,  0.1094,  1.4987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9486,  0.2285, -1.2758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0791, -0.1260,  2.8160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8545, -0.1368,  2.4468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2184,  1.1062,  0.1072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3835,  0.3143,  2.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6139, -0.4829,  3.9630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7200, -0.1869, -1.5973]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1881,  0.7341,  0.5486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0191, -0.3984, -2.6830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2239,  0.2026,  0.0940]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3669,  1.0569,  1.8551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7772, -0.0626, -2.8649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3481, -0.2304,  3.2317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1403,  0.4384, -2.6868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1000, -0.5377, -2.5332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2629, -0.5618, -2.8489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1938,  0.0761, -2.2694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4814, -0.4101,  3.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3338, -0.8225,  4.0247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7337,  0.3073,  0.5143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9069,  0.3176,  1.8913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9278,  1.0956,  0.7776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7731, -0.5532,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1680, -0.3528, -2.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6632,  0.5331, -1.2166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6959,  0.1012,  0.7537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4119,  0.2677, -0.6890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8661, -0.3836, -2.5584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6380, -0.4565,  3.8105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6677, -0.0930,  3.2348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0682,  0.1955, -2.4324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1100,  0.3256, -0.1916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5712,  0.5139,  2.3088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5883,  0.2919,  2.7402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7070,  0.2742,  0.5259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3573,  0.2294, -2.6814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1036, -0.4573,  3.2130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0422,  0.3573,  0.8428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5771, -0.2995,  3.5757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3071,  0.3303, -0.6113]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9895, -0.5726, -2.4826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6265,  0.2958, -2.0756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7664,  0.0475, -0.7997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1597, -0.3188,  3.0449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6002,  0.2133,  0.5378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6770, -0.1990, -2.5684]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4927,  0.1372, -0.6337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4204,  0.3950, -1.8435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3321, -0.8188,  3.9872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5078,  0.7426, -2.2782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2856,  0.7370, -1.0373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9406,  0.1711, -2.0439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9473, -0.3665, -2.5490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7292, -0.1735,  2.2925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1769, -0.5745,  3.6310]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3071, -0.5005, -2.8498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5825, -0.4809,  3.8544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4252,  0.9200,  0.6929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2991,  0.4981, -1.9131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6857, -0.1040, -1.6869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7165,  0.5271, -2.4047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1071, -0.0955, -2.0016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7907,  0.2424, -1.1045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4175, -0.1381,  2.0083]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7968,  0.3223,  2.8121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4752,  0.9246,  1.4230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5956,  0.8207, -2.4073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7603,  0.1369,  1.8337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9193,  0.1886, -2.0621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1925,  0.1224, -0.2481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9433, -0.3425,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6122,  0.0456,  0.9590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6766,  0.2544, -0.9706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1905,  0.4875, -0.6630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6711,  0.3364, -0.9518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7086,  0.0643,  2.0173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7070,  0.2742,  0.5259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3573,  0.2294, -2.6814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1036, -0.4573,  3.2130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4814, -0.4101,  3.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3338, -0.8225,  4.0247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7337,  0.3073,  0.5143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9069,  0.3176,  1.8913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9278,  1.0956,  0.7776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7731, -0.5532,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1680, -0.3528, -2.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6632,  0.5331, -1.2166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6959,  0.1012,  0.7537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3389,  0.2022, -2.7159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4205, -0.7281, -2.7186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4113,  0.0836,  2.8103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9260, -0.4416, -2.5508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6930,  0.4506, -2.2562]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2070,  0.0582,  1.4039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7864,  0.2133, -2.1411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0203, -0.4705, -2.5499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4548, -0.8344,  4.2262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2410,  0.3416,  1.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4675,  0.1018,  2.8328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6766, -0.3972, -2.3085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7398, -0.2039, -2.5348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7186,  0.4321,  2.6366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9818, -0.9929,  3.9201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0921,  0.1533, -2.3448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4371, -0.0405, -2.4240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4371, -0.0405, -2.4240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0970,  0.3018,  0.9375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8535,  0.5620, -2.2827]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1502, -0.5253, -2.7006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7998,  0.3908,  0.4878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7226,  0.6713,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1541,  0.5058, -1.7519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8853, -0.4949, -2.4296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7179, -0.0929,  1.0215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1279,  0.3707,  0.9677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9008, -0.6265,  3.3481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0166,  1.0873, -0.8522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9565,  1.3714, -0.4236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5077,  1.3196,  1.5583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6275,  0.2076, -0.8079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3270, -0.0676,  3.0645]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2886,  0.2077,  2.3383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6536, -0.2282,  3.6398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2972,  0.5948,  0.8370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5499,  0.5163,  1.4784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8928,  0.3771,  2.8369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4209,  0.0173,  3.0678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4858, -0.1675,  3.3399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6303,  0.1212,  2.8979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1512,  0.8835,  1.3259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5596, -0.2399, -2.3988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8204,  0.7587,  1.2197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0652,  1.2153, -0.1486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1355, -0.0920,  1.4853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6317,  0.3189,  2.8225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1864,  0.4811, -0.1109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3879,  0.1549,  2.5938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1551,  1.4887,  0.4884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2736, -0.1276,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1251,  0.4474,  2.0068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6611,  0.8876,  0.9142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4696,  0.9095, -0.4952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8058,  0.5791,  1.3823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0172,  0.3653, -2.3672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0409, -0.1371, -1.9597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6120,  0.2148,  2.8867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5837,  0.0189, -2.8159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6060,  1.0053, -1.7952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5152, -0.2056, -2.2179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6525,  0.6361,  1.1496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4582,  0.7117,  1.1419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9734,  0.8489, -1.7446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5077, -0.2213, -2.3588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5664, -0.0414,  3.1353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9421,  0.1314, -1.0999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2169, -0.1219,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6601, -0.2498,  3.4770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4923,  0.2335, -2.7202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4227e+00, -1.1821e-03, -1.4844e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0456,  1.3421,  0.6302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6671,  0.7200, -1.4961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7527,  0.5336,  1.6427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4034, -0.5954,  3.7297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5776, -0.3027, -2.3218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7608,  0.0678,  2.0931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7335, -0.3485, -1.4281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1947, -0.0653,  2.5882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2799, -0.8332,  4.0492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4522,  0.0878,  0.5907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7076, -0.1651,  3.5379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9066, -0.2216,  2.6234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2110,  0.5784, -0.1460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7333,  0.2335,  1.9636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2193,  0.2643,  0.0249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1690, -0.8376,  3.9682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2606,  0.9477, -0.5640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3371,  0.1091,  1.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9524, -0.5314, -2.4904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3776, -0.7948, -2.6633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9430, -0.0702,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1011,  0.9193, -0.8878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6050, -0.0786,  2.0630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8397, -0.2998, -2.4666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9433,  0.0750,  1.0721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5819, -0.4051, -2.2591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6603, -0.3045, -2.5124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7616, -0.4530, -2.3203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9883, -0.4418, -2.6005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2823,  0.4302, -0.1130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9477, -0.1261,  3.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8465, -0.5391,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7612,  0.1883,  0.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8750,  0.0175,  3.3222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5216,  0.4202, -2.0439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7554, -0.2147, -2.6580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3677,  0.1955, -0.5626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0838,  0.3732, -0.4207]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3967,  0.3601, -1.8218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2585,  0.1269, -2.4328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4056,  0.1295, -2.4495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9256,  1.1989,  0.9855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3960,  0.8933, -0.3444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4373,  0.1029, -1.6148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1311,  0.2460,  1.1206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4808,  0.1484,  0.4860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9172,  0.0986, -1.8337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2198,  0.2210, -0.4068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2952, -0.7474, -2.5066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6442, -0.1486, -2.3911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0409, -0.1371, -1.9597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2205, -0.8861, -2.3503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2814,  0.0375, -2.0863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7402,  0.5796,  0.2410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0470, -0.6511, -2.5345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6259,  0.0922,  2.8800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2538,  0.6614, -0.2599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4976,  0.5049,  2.3707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5365, -0.6252,  3.9591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5347,  0.1417,  2.8115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9421,  0.1314, -1.0999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7987, -0.0149, -2.8912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0809,  0.5255,  0.6708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6130,  0.3152,  2.5728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6335,  0.0838,  3.0422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3958,  0.2154, -0.5602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3366, -0.6220,  3.7733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3360, -0.1854, -2.2407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3801,  0.2165, -1.6886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4705,  1.0253, -0.4857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7608,  0.0678,  2.0931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6120,  0.2148,  2.8867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5837,  0.0189, -2.8159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6060,  1.0053, -1.7952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5152, -0.2056, -2.2179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6525,  0.6361,  1.1496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4582,  0.7117,  1.1419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9734,  0.8489, -1.7446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5077, -0.2213, -2.3588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5664, -0.0414,  3.1353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5506, -0.4953,  3.8386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7335, -0.3485, -1.4281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1947, -0.0653,  2.5882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2799, -0.8332,  4.0492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4522,  0.0878,  0.5907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7076, -0.1651,  3.5379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9066, -0.2216,  2.6234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2110,  0.5784, -0.1460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7333,  0.2335,  1.9636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2193,  0.2643,  0.0249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3591, -0.3055, -1.7948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 3\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2647, -0.7403,  3.8546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2963, -0.8102,  4.0323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1087,  0.1839,  2.4189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8692,  0.0761, -2.0685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3815, -0.0728,  3.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7861,  0.7250,  0.2679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1760, -0.6230, -2.5936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1467,  0.6715, -0.6610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3710, -0.1798,  2.2419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2606,  0.9477, -0.5640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3371,  0.1091,  1.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9524, -0.5314, -2.4904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3776, -0.7948, -2.6633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9430, -0.0702,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1011,  0.9193, -0.8878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6050, -0.0786,  2.0630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8397, -0.2998, -2.4666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2096, -0.0332,  1.6541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8497,  0.3704,  2.7558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6454, -0.1235,  3.3284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8241,  0.2932,  2.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8821,  0.3824,  2.7450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6133,  0.0213,  3.0434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4438,  0.6306,  2.0009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6057,  0.5248,  2.2415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4588, -0.0948,  3.1013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5819, -0.4051, -2.2591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6603, -0.3045, -2.5124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7616, -0.4530, -2.3203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9883, -0.4418, -2.6005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2823,  0.4302, -0.1130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9477, -0.1261,  3.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8465, -0.5391,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7612,  0.1883,  0.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8750,  0.0175,  3.3222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8370,  0.1007,  3.1377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2169, -0.1219,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6601, -0.2498,  3.4770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4923,  0.2335, -2.7202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4227e+00, -1.1821e-03, -1.4844e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0456,  1.3421,  0.6302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6671,  0.7200, -1.4961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7527,  0.5336,  1.6427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4034, -0.5954,  3.7297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5776, -0.3027, -2.3218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9433,  0.0750,  1.0721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3692,  1.1203,  1.3911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4885,  0.1252, -0.5830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2006, -0.4764, -2.7015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5656, -0.6993,  4.0396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1637,  0.2966, -0.3891]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4329,  0.1494,  0.4188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5993, -0.0900, -2.6334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0095,  0.8155,  1.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5589,  1.7030, -0.0601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4465, -0.2264,  3.2627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6398,  0.0435,  3.0075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4329,  0.1494,  0.4188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5993, -0.0900, -2.6334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0095,  0.8155,  1.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5589,  1.7030, -0.0601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4465, -0.2264,  3.2627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6398,  0.0435,  3.0075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1842,  0.0284,  0.2722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7238,  1.5506, -1.0201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7040, -0.2630, -2.3975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2337, -0.3092,  3.1191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Run polarity score on the entire dataset\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "                text = row['Reviews']\n",
    "                rate = row['Rating']\n",
    "                vader_result = sia.polarity_scores(text)\n",
    "                #rename\n",
    "                vader_result_rename = {}\n",
    "                for key, value in vader_result.items():\n",
    "                        vader_result_rename[f\"vader_{key}\"] = value\n",
    "                        \n",
    "                roberta_result = polarity_scores_roberta(text)\n",
    "                both= {**vader_result_rename,  **roberta_result}\n",
    "                results[rate] = both\n",
    "        except RuntimeError:\n",
    "                print(f\"Broke for the rating: {rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    neg    neu    pos  compound Brand Name  \\\n",
      "0   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "1   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "2   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "3   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "4   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "\n",
      "                                        Product Name   Price  Rating  \\\n",
      "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
      "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
      "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "\n",
      "                                             Reviews  Review Votes  \n",
      "0  I feel so LUCKY to have found this used (phone...           1.0  \n",
      "1  nice phone, nice up grade from my pantach revu...           0.0  \n",
      "2                                       Very pleased           0.0  \n",
      "3  It works good but it goes slow sometimes but i...           0.0  \n",
      "4  Great phone to replace my lost phone. The only...           0.0  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check the output\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresults_df\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:242\u001b[0m, in \u001b[0;36mOpsMixin.__pow__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pow__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:7913\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7910\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other, axis, flex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   7912\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 7913\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_frame_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:7956\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[1;34m(self, right, func, axis)\u001b[0m\n\u001b[0;32m   7950\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mequals(right\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m   7951\u001b[0m     \u001b[38;5;66;03m# TODO: The previous assertion `assert right._indexed_same(self)`\u001b[39;00m\n\u001b[0;32m   7952\u001b[0m     \u001b[38;5;66;03m#  fails in cases with empty columns reached via\u001b[39;00m\n\u001b[0;32m   7953\u001b[0m     \u001b[38;5;66;03m#  _frame_arith_method_with_reindex\u001b[39;00m\n\u001b[0;32m   7954\u001b[0m \n\u001b[0;32m   7955\u001b[0m     \u001b[38;5;66;03m# TODO operate_blockwise expects a manager of the same type\u001b[39;00m\n\u001b[1;32m-> 7956\u001b[0m     bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperate_blockwise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\u001b[39;49;00m\n\u001b[0;32m   7958\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[0;32m   7959\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"ArrayManager\"\u001b[39;49;00m\n\u001b[0;32m   7960\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 1 to \"operate_blockwise\" of \"BlockManager\" has\u001b[39;49;00m\n\u001b[0;32m   7961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[0;32m   7962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"BlockManager\"\u001b[39;49;00m\n\u001b[0;32m   7963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   7964\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(bm, axes\u001b[38;5;241m=\u001b[39mbm\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   7968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, Series) \u001b[38;5;129;01mand\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   7969\u001b[0m     \u001b[38;5;66;03m# axis=1 means we want to operate row-by-row\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1511\u001b[0m, in \u001b[0;36mBlockManager.operate_blockwise\u001b[1;34m(self, other, array_op)\u001b[0m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moperate_blockwise\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: BlockManager, array_op) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BlockManager:\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124;03m    Apply array_op blockwise with another (aligned) BlockManager.\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperate_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\ops.py:65\u001b[0m, in \u001b[0;36moperate_blockwise\u001b[1;34m(left, right, array_op)\u001b[0m\n\u001b[0;32m     63\u001b[0m res_blks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lvals, rvals, locs, left_ea, right_ea, rblk \u001b[38;5;129;01min\u001b[39;00m _iter_block_pairs(left, right):\n\u001b[1;32m---> 65\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43marray_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     67\u001b[0m         left_ea\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_ea\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(res_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_1d_only_ea_dtype(res_values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     71\u001b[0m     ):\n\u001b[0;32m     72\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Reset index and rename columns to align with the structure you want\n",
    "results_df = results_df.reset_index().rename(columns={\"index\": \"Id\"})\n",
    "\n",
    "# Now perform the merge on the appropriate column, assuming \"ProductId\" is the key\n",
    "results_df = results_df.merge(df, how=\"left\")  \n",
    "\n",
    "# Check the output\n",
    "print(results_df.head())\n",
    "print (**df **results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'neg', 'neu', 'pos', 'compound', 'Brand Name', 'Product Name',\n",
      "       'Price', 'Rating', 'Reviews', 'Review Votes'],\n",
      "      dtype='object')\n",
      "Index(['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews',\n",
      "       'Review Votes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(results_df.columns)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    neg    neu    pos  compound Brand Name  \\\n",
      "0   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "1   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "2   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "3   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "4   0  0.015  0.796  0.189    0.8783    Samsung   \n",
      "\n",
      "                                        Product Name   Price  Rating  \\\n",
      "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
      "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       5   \n",
      "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  199.99       4   \n",
      "\n",
      "                                             Reviews  Review Votes  \n",
      "0  I feel so LUCKY to have found this used (phone...           1.0  \n",
      "1  nice phone, nice up grade from my pantach revu...           0.0  \n",
      "2                                       Very pleased           0.0  \n",
      "3  It works good but it goes slow sometimes but i...           0.0  \n",
      "4  Great phone to replace my lost phone. The only...           0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'neg', 'neu', 'pos', 'compound', 'Brand Name', 'Product Name',\n",
       "       'Price', 'Rating', 'Reviews', 'Review Votes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Scores between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
