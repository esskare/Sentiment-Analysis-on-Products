{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\n",
      "(413840, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re  # Import re for regex-based tokenization\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# # Define the custom tokenization function\n",
    "# def simple_tokenize(text):\n",
    "#     tokens = text.split()  # Basic tokenization by splitting on whitespace\n",
    "#     return tokens\n",
    "\n",
    "# Reading data \n",
    "df = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
    "\n",
    "# Show the specified index on the 'Reviews' row\n",
    "print(df['Reviews'].values[0])  \n",
    "\n",
    "# Print the shape of the dataset.\n",
    "print(df.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df = df.head(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUICK EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick EDA: Plot the count of reviews by stars\n",
    "ax = df['Rating'].value_counts().sort_index().plot(\n",
    "    kind='bar', \n",
    "    title='Count of Reviews by stars',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "ax.set_xlabel('Review Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC NLTK TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fiance', 'had', 'this', 'phone', 'previously', ',', 'but', 'caused', 'many', 'problems', '.', 'So', ',', 'of', 'course', ',', 'we', 'decided', 'to', 'browse', 'amazon', 'for', 'a', 'replacement', 'til', \"'\", 'our', 'contract', 'is', 'up', '!', '&', 'so', 'far', 'so', 'good', '!']\n",
      "(S\n",
      "  My/PRP$\n",
      "  fiance/NN\n",
      "  had/VBD\n",
      "  this/DT\n",
      "  phone/NN\n",
      "  previously/RB\n",
      "  ,/,\n",
      "  but/CC\n",
      "  caused/VBD\n",
      "  many/JJ\n",
      "  problems/NNS\n",
      "  ./.\n",
      "  So/RB\n",
      "  ,/,\n",
      "  of/IN\n",
      "  course/NN\n",
      "  ,/,\n",
      "  we/PRP\n",
      "  decided/VBD\n",
      "  to/TO\n",
      "  browse/VB\n",
      "  amazon/NN\n",
      "  for/IN\n",
      "  a/DT\n",
      "  replacement/NN\n",
      "  til/NN\n",
      "  '/''\n",
      "  our/PRP$\n",
      "  contract/NN\n",
      "  is/VBZ\n",
      "  up/RP\n",
      "  !/.\n",
      "  &/CC\n",
      "  so/RB\n",
      "  far/RB\n",
      "  so/RB\n",
      "  good/JJ\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "# Basic NLTK Tokenization\n",
    "example = df['Reviews'][10]\n",
    "tokens = nltk.word_tokenize(example)  # Tokenize the 'example' variable\n",
    "print(tokens)\n",
    "tokens[:10] #first 10 words\n",
    "tagged= nltk.pos_tag(tokens)\n",
    "tagged[:10]\n",
    "#putting them into entities\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADERS MODEL\n",
    "- Uses a \"cluster of words\" approach\n",
    "- Stop words are removed e.g. and, or\n",
    "- Each word is combined to a total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f75574577104ef7ae17b8cd2bb13b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Ensure 'Reviews' column is string type and handle missing values\n",
    "df['Reviews'] = df['Reviews'].astype(str)  # Convert all entries to string\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "# Run polarity score on the entire dataset\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['Reviews']\n",
    "    name = row['Brand Name']\n",
    "    \n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        polarity_scores = sia.polarity_scores(text)\n",
    "        polarity_scores['Brand Name'] = name  # Add 'Brand Name' to the results\n",
    "        results.append(polarity_scores)\n",
    "    else:\n",
    "        results.append({\"Brand Name\": name, \"error\": \"Invalid text\"})\n",
    "\n",
    "# # Convert results into a DataFrame\n",
    "# vaders = pd.DataFrame(results)\n",
    "\n",
    "# # Merge the results with the original DataFrame\n",
    "# vaders = vaders.merge(df, how='left', on='Brand Name')\n",
    "\n",
    "# # Display the first few rows of the merged DataFrame\n",
    "# vaders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Brand Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12016\\4135104059.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Id\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Brand Name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Brand Name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swanjuki\\Documents\\Kare\\Sentiment-Analysis-on-Products\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Brand Name'"
     ]
    }
   ],
   "source": [
    "vaders = pd.DataFrame(results).T\n",
    "vaders = vaders.reset_index().rename(columns={\"index\": \"Id\"})\n",
    "vaders = vaders.merge(df, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>I originally was using the Samsung S2 Galaxy f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>This is a great product it came after two days...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>These guys are the best! I had a little situat...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  vader_neg  vader_neu  vader_pos  vader_compound  roberta_neg  \\\n",
       "0       5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "1       5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "2       5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "3       5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "4       5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "\n",
       "   roberta_neu  roberta_pos  \\\n",
       "0     0.031279     0.964155   \n",
       "1     0.031279     0.964155   \n",
       "2     0.031279     0.964155   \n",
       "3     0.031279     0.964155   \n",
       "4     0.031279     0.964155   \n",
       "\n",
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "                                             Reviews  Review Votes  \n",
       "0  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1                                       Very pleased           0.0  \n",
       "2  I originally was using the Samsung S2 Galaxy f...           0.0  \n",
       "3  This is a great product it came after two days...           0.0  \n",
       "4  These guys are the best! I had a little situat...           2.0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No we have sentiment score and metadata\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT VADERS RESULTS\n",
    "- bar plot of the compound of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(data = vaders, x = 'Rating', y='compound')\n",
    "ax.set_title('Compound Score by Amazon Star Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS, Neg, Neu Scores of the star Ratings\n",
    "\n",
    "Observation and Assumptions respectively\n",
    "- The Plots look similar to one another.\n",
    "\n",
    "- Vader tends to pick up on a lot of cluster words and may not be finely tuned to nuances\n",
    "- There are many nuances of specific texts leading to similar sentiment scores across different ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "sns.barplot(data = vaders, x = 'Rating', y='neg', ax=axs[0])\n",
    "sns.barplot(data = vaders, x = 'Rating', y='neu', ax=axs[1])\n",
    "sns.barplot(data = vaders, x = 'Rating', y='pos', ax=axs[2])\n",
    "axs[0].set_title('Negative')\n",
    "axs[1].set_title('Neutral')\n",
    "axs[2].set_title('Positive')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROBERTA PRETRAINED MODEL\n",
    "- Uses a model trained of a large corpus of data\n",
    "- Transformer model accounts for the words but also the context related to other words\n",
    "- Applying the trained weights to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ROBERTA MODEL\n",
    "import tf_keras as keras\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "#Specific model trained\n",
    "MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sia.polarity_scores(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7586,  0.0125,  0.9647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "{'roberta_neg': 0.11408854, 'roberta_neu': 0.24666648, 'roberta_pos': 0.6392449}\n"
     ]
    }
   ],
   "source": [
    "# encoded text\n",
    "import numpy as np\n",
    "encoded_text = tokenizer(example, return_tensors = \"pt\")\n",
    "output = model(**encoded_text) #tensor with our results\n",
    "print(output) \n",
    "scores = output[0][0].detach().numpy() #convert to numpy array\n",
    "scores = softmax(scores) #apply softmax\n",
    "scores_dict = {\n",
    "    'roberta_neg': scores[0],\n",
    "    'roberta_neu': scores[1],\n",
    "    'roberta_pos': scores[2]\n",
    "}\n",
    "print(scores_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors = \"pt\")\n",
    "    output = model(**encoded_text) #tensor with our results\n",
    "    print(output) \n",
    "    scores = output[0][0].detach().numpy() #convert to numpy array\n",
    "    scores = softmax(scores) #apply softmax\n",
    "    scores_dict = {\n",
    "        'roberta_neg': scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b321b99f01714911942a6993a141b4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9108, -0.5009,  3.0563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2665, -0.7459,  3.8441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6510,  0.1087,  3.1351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0012, -0.7938,  3.6114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.8940, -0.0059,  1.2029]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2062, -0.0542, -2.2693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9189,  0.2051, -1.2075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6769, -0.4487, -2.2836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6853, -0.1345,  2.0856]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3583, -0.0849,  1.8801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7586,  0.0125,  0.9647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8129, -0.4314,  2.8464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2735, -0.5568,  3.6031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0224, -0.0515, -2.0543]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9416, -0.1758,  2.6924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0538,  0.2407, -0.1688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2343, -0.8584,  3.8957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0642, -0.6929,  3.5008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6654, -0.5435,  2.9444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0886,  0.1306,  1.1260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4495,  0.4439, -0.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2042, -0.3214,  3.1593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5699,  0.3201, -2.0665]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3225, -0.8004,  3.8516]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3037, -0.3486,  3.2875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2775, -0.9299,  4.2147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0243,  0.0673,  2.2036]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1179, -0.3396,  3.2096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3234,  0.4480,  0.9688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5481,  0.0086, -1.6715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1720,  1.3665,  1.2374]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9388,  0.2534,  2.3417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5922, -0.0982, -2.4969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3694, -0.8725,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7696, -0.0204,  0.9349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5006, -0.8089,  4.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7525, -0.4431,  2.9616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7828,  0.5873,  0.2422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2906, -0.2284,  3.2618]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1200,  0.1858,  2.5783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6328, -0.6300,  4.0745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8010,  0.5964, -1.4016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3826,  0.0188,  0.6666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9682, -0.7445,  3.6183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6168, -0.6128,  3.9381]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7645,  0.6223,  2.3120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8445, -0.1709,  3.5810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5647,  0.2155,  2.7969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3291, -0.3228,  3.4832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4142,  0.1316,  1.4320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1318,  1.0884, -0.9558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5541,  0.0840, -1.8226]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9911,  0.1265,  3.2424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6890, -0.0960,  3.2772]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7989, -0.1146,  3.3425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8098,  0.7268,  1.2729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9396,  0.5231,  1.7694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5281,  0.1547,  2.9814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3030, -0.4073,  3.4522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7442, -0.3284,  3.7037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2995,  0.0425,  1.5016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6427,  1.0813,  1.5591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5953,  0.2559,  1.4737]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2455,  0.6638, -1.9406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4790,  0.0449,  2.8387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.8783,  1.4494, -0.5699]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3224, -0.3982,  3.3853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3003,  0.0551,  1.4866]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 2\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3393, -0.4660,  3.5175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1316, -0.1726,  2.7160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5182,  0.0922,  0.5764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2639, -0.5733,  3.4787]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6348,  0.3669,  1.3936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5087, -0.7529,  4.0472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5518, -0.0552, -2.5703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6212, -0.0070,  3.0368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8739, -0.2380,  2.6472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3928,  0.1614,  0.2711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7193, -0.0617,  2.4461]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3473,  0.2499, -0.6188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7455, -0.5295,  4.0284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6471, -0.0918, -1.6617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5837,  0.2653, -0.7167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2410,  0.3416,  1.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6736,  0.2203, -1.9080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8996,  0.1287,  2.3260]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5217,  0.9501,  1.0332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2869,  0.3720, -2.6638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3214, -0.6663,  3.7871]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6132,  0.0951,  3.0048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4663, -0.2205,  3.1799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0817,  0.0316, -2.2024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3120,  0.6694, -0.1834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4359, -0.1711,  2.0778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5350,  0.1993,  2.7807]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1302,  0.4923,  0.8096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6239, -0.4413,  3.7629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6275,  0.3374, -0.9838]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4731, -0.1841,  3.2331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4807,  0.2076,  1.4352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9406,  0.2795, -1.3773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1639,  0.3132,  2.2738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4067,  0.0864,  2.8353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1388,  0.1131,  2.5548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6869,  0.3198,  0.4594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2420,  0.8011, -2.1647]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7898,  0.1367,  1.9305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5156, -0.3167,  3.4288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3408,  0.7240, -0.2279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8043,  0.0915,  2.1939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 5\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5719, -0.5103,  3.8171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7299, -0.2749,  3.5566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1746, -0.2693,  1.8140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0848,  0.1347, -1.2147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1171, -0.1491, -2.0626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3035,  0.6018,  1.3568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7769,  1.0268,  0.8429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5841, -0.3018,  3.5620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7812,  0.0395,  1.1541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0664, -0.3517,  3.1525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0158, -0.5613, -2.5145]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1822,  0.9579,  1.6937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2211,  0.0330, -1.2818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3370,  0.1579,  2.7191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4366,  0.0415,  2.8584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2904,  0.7204, -0.3723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2907,  0.6577, -0.2187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5005,  1.5223, -1.0166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1302,  0.0962, -1.3451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0349,  0.0813,  1.0727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4005,  0.3125, -0.7398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8972, -0.3140,  2.8109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2350, -0.5875,  3.6318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3844,  0.0552, -2.4408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1952, -0.0846,  1.7677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3327,  0.1422,  2.5745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5528, -0.0681, -2.6333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9507,  0.1970,  0.9200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1192, -0.0259,  1.4657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3909, -0.5227,  3.6433]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1914, -0.5352, -2.5305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6043,  0.8007, -1.3935]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1554,  0.2634, -2.5007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9507, -0.3976,  2.8746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7327, -0.3890,  3.7022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4115, -0.6234,  3.8958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1627,  0.5740, -0.6397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6159,  0.0251,  1.8720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9270, -0.1841,  1.4465]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4283,  0.2519,  2.6460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6288,  0.0938, -0.6371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3521,  1.2858, -1.4397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9475,  0.1096,  1.0521]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.2064,  0.3867,  3.3155]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6211, -0.2326,  2.3682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3527,  0.1094,  1.4987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9486,  0.2285, -1.2758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0791, -0.1260,  2.8160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8545, -0.1368,  2.4468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2184,  1.1062,  0.1072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3835,  0.3143,  2.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6139, -0.4829,  3.9630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7200, -0.1869, -1.5973]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1881,  0.7341,  0.5486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0191, -0.3984, -2.6830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2239,  0.2026,  0.0940]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3669,  1.0569,  1.8551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7772, -0.0626, -2.8649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3481, -0.2304,  3.2317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1403,  0.4384, -2.6868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1000, -0.5377, -2.5332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2629, -0.5618, -2.8489]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1938,  0.0761, -2.2694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4814, -0.4101,  3.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3338, -0.8225,  4.0247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7337,  0.3073,  0.5143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9069,  0.3176,  1.8913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9278,  1.0956,  0.7776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7731, -0.5532,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1680, -0.3528, -2.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6632,  0.5331, -1.2166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6959,  0.1012,  0.7537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4119,  0.2677, -0.6890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8661, -0.3836, -2.5584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6380, -0.4565,  3.8105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6677, -0.0930,  3.2348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0682,  0.1955, -2.4324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1100,  0.3256, -0.1916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5712,  0.5139,  2.3088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5883,  0.2919,  2.7402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7070,  0.2742,  0.5259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3573,  0.2294, -2.6814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1036, -0.4573,  3.2130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0422,  0.3573,  0.8428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5771, -0.2995,  3.5757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3071,  0.3303, -0.6113]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9895, -0.5726, -2.4826]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6265,  0.2958, -2.0756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7664,  0.0475, -0.7997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1597, -0.3188,  3.0449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6002,  0.2133,  0.5378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6770, -0.1990, -2.5684]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4927,  0.1372, -0.6337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4204,  0.3950, -1.8435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3321, -0.8188,  3.9872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5078,  0.7426, -2.2782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2856,  0.7370, -1.0373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9406,  0.1711, -2.0439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9473, -0.3665, -2.5490]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7292, -0.1735,  2.2925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1769, -0.5745,  3.6310]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3071, -0.5005, -2.8498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5825, -0.4809,  3.8544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4252,  0.9200,  0.6929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2991,  0.4981, -1.9131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6857, -0.1040, -1.6869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7165,  0.5271, -2.4047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1071, -0.0955, -2.0016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7907,  0.2424, -1.1045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4175, -0.1381,  2.0083]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7968,  0.3223,  2.8121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4752,  0.9246,  1.4230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5956,  0.8207, -2.4073]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7603,  0.1369,  1.8337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9193,  0.1886, -2.0621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1925,  0.1224, -0.2481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9433, -0.3425,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6122,  0.0456,  0.9590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6766,  0.2544, -0.9706]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1905,  0.4875, -0.6630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6711,  0.3364, -0.9518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7086,  0.0643,  2.0173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7070,  0.2742,  0.5259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3573,  0.2294, -2.6814]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1036, -0.4573,  3.2130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4814, -0.4101,  3.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7508,  0.9459,  0.0571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3338, -0.8225,  4.0247]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7337,  0.3073,  0.5143]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9069,  0.3176,  1.8913]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9278,  1.0956,  0.7776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7731, -0.5532,  4.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1680, -0.3528, -2.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6632,  0.5331, -1.2166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6959,  0.1012,  0.7537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3389,  0.2022, -2.7159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4205, -0.7281, -2.7186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4113,  0.0836,  2.8103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9260, -0.4416, -2.5508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6930,  0.4506, -2.2562]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2070,  0.0582,  1.4039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7864,  0.2133, -2.1411]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0203, -0.4705, -2.5499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4548, -0.8344,  4.2262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2410,  0.3416,  1.2053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4675,  0.1018,  2.8328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6766, -0.3972, -2.3085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7398, -0.2039, -2.5348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7186,  0.4321,  2.6366]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9818, -0.9929,  3.9201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0921,  0.1533, -2.3448]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4371, -0.0405, -2.4240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4371, -0.0405, -2.4240]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0970,  0.3018,  0.9375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8535,  0.5620, -2.2827]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1502, -0.5253, -2.7006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7998,  0.3908,  0.4878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7226,  0.6713,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1541,  0.5058, -1.7519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4578, -0.7692,  4.0785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8853, -0.4949, -2.4296]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7179, -0.0929,  1.0215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1279,  0.3707,  0.9677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9008, -0.6265,  3.3481]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0166,  1.0873, -0.8522]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9565,  1.3714, -0.4236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5077,  1.3196,  1.5583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6275,  0.2076, -0.8079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3270, -0.0676,  3.0645]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2886,  0.2077,  2.3383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6536, -0.2282,  3.6398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2972,  0.5948,  0.8370]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5499,  0.5163,  1.4784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8928,  0.3771,  2.8369]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4209,  0.0173,  3.0678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4858, -0.1675,  3.3399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6303,  0.1212,  2.8979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1512,  0.8835,  1.3259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5596, -0.2399, -2.3988]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8204,  0.7587,  1.2197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0652,  1.2153, -0.1486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1355, -0.0920,  1.4853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6317,  0.3189,  2.8225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1864,  0.4811, -0.1109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3879,  0.1549,  2.5938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1551,  1.4887,  0.4884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2736, -0.1276,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1251,  0.4474,  2.0068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6611,  0.8876,  0.9142]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4696,  0.9095, -0.4952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8058,  0.5791,  1.3823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0172,  0.3653, -2.3672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0409, -0.1371, -1.9597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6120,  0.2148,  2.8867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5837,  0.0189, -2.8159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6060,  1.0053, -1.7952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5152, -0.2056, -2.2179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6525,  0.6361,  1.1496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4582,  0.7117,  1.1419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9734,  0.8489, -1.7446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5077, -0.2213, -2.3588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5664, -0.0414,  3.1353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9421,  0.1314, -1.0999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2169, -0.1219,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6601, -0.2498,  3.4770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4923,  0.2335, -2.7202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4227e+00, -1.1821e-03, -1.4844e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0456,  1.3421,  0.6302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6671,  0.7200, -1.4961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7527,  0.5336,  1.6427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4034, -0.5954,  3.7297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5776, -0.3027, -2.3218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7608,  0.0678,  2.0931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7335, -0.3485, -1.4281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1947, -0.0653,  2.5882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2799, -0.8332,  4.0492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4522,  0.0878,  0.5907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7076, -0.1651,  3.5379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9066, -0.2216,  2.6234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2110,  0.5784, -0.1460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7333,  0.2335,  1.9636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2193,  0.2643,  0.0249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1690, -0.8376,  3.9682]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2606,  0.9477, -0.5640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3371,  0.1091,  1.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9524, -0.5314, -2.4904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3776, -0.7948, -2.6633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9430, -0.0702,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1011,  0.9193, -0.8878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6050, -0.0786,  2.0630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8397, -0.2998, -2.4666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9433,  0.0750,  1.0721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5819, -0.4051, -2.2591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6603, -0.3045, -2.5124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7616, -0.4530, -2.3203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9883, -0.4418, -2.6005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2823,  0.4302, -0.1130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9477, -0.1261,  3.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8465, -0.5391,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7612,  0.1883,  0.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8750,  0.0175,  3.3222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5216,  0.4202, -2.0439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7554, -0.2147, -2.6580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3677,  0.1955, -0.5626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0838,  0.3732, -0.4207]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3967,  0.3601, -1.8218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2585,  0.1269, -2.4328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4056,  0.1295, -2.4495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9256,  1.1989,  0.9855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3960,  0.8933, -0.3444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4373,  0.1029, -1.6148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1311,  0.2460,  1.1206]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4808,  0.1484,  0.4860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9172,  0.0986, -1.8337]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2198,  0.2210, -0.4068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2952, -0.7474, -2.5066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6442, -0.1486, -2.3911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5345,  0.5142,  0.2026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0409, -0.1371, -1.9597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2205, -0.8861, -2.3503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2814,  0.0375, -2.0863]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7402,  0.5796,  0.2410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0470, -0.6511, -2.5345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6259,  0.0922,  2.8800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2538,  0.6614, -0.2599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4976,  0.5049,  2.3707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5365, -0.6252,  3.9591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5347,  0.1417,  2.8115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9421,  0.1314, -1.0999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7987, -0.0149, -2.8912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0809,  0.5255,  0.6708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6130,  0.3152,  2.5728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6335,  0.0838,  3.0422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3958,  0.2154, -0.5602]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3366, -0.6220,  3.7733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3360, -0.1854, -2.2407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3801,  0.2165, -1.6886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4705,  1.0253, -0.4857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7608,  0.0678,  2.0931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6120,  0.2148,  2.8867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5837,  0.0189, -2.8159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6060,  1.0053, -1.7952]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5152, -0.2056, -2.2179]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6525,  0.6361,  1.1496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4582,  0.7117,  1.1419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9734,  0.8489, -1.7446]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5077, -0.2213, -2.3588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5664, -0.0414,  3.1353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5506, -0.4953,  3.8386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7335, -0.3485, -1.4281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1947, -0.0653,  2.5882]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2799, -0.8332,  4.0492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4522,  0.0878,  0.5907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7076, -0.1651,  3.5379]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9066, -0.2216,  2.6234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2110,  0.5784, -0.1460]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7333,  0.2335,  1.9636]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2193,  0.2643,  0.0249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3591, -0.3055, -1.7948]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Broke for the rating: 3\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2647, -0.7403,  3.8546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2963, -0.8102,  4.0323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1087,  0.1839,  2.4189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8692,  0.0761, -2.0685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3815, -0.0728,  3.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7861,  0.7250,  0.2679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1760, -0.6230, -2.5936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1467,  0.6715, -0.6610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3710, -0.1798,  2.2419]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2606,  0.9477, -0.5640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.3371,  0.1091,  1.6464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9524, -0.5314, -2.4904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3776, -0.7948, -2.6633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2369,  0.4529,  1.0687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.9430, -0.0702,  2.3373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1011,  0.9193, -0.8878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6050, -0.0786,  2.0630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8397, -0.2998, -2.4666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2096, -0.0332,  1.6541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8497,  0.3704,  2.7558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6454, -0.1235,  3.3284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8241,  0.2932,  2.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8821,  0.3824,  2.7450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6133,  0.0213,  3.0434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4438,  0.6306,  2.0009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6057,  0.5248,  2.2415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4588, -0.0948,  3.1013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5819, -0.4051, -2.2591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6603, -0.3045, -2.5124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7616, -0.4530, -2.3203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9883, -0.4418, -2.6005]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2823,  0.4302, -0.1130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9477, -0.1261,  3.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8465, -0.5391,  2.9868]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7612,  0.1883,  0.5927]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8750,  0.0175,  3.3222]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.8370,  0.1007,  3.1377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2169, -0.1219,  2.9788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6601, -0.2498,  3.4770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4923,  0.2335, -2.7202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4227e+00, -1.1821e-03, -1.4844e+00]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0456,  1.3421,  0.6302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6671,  0.7200, -1.4961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7527,  0.5336,  1.6427]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4034, -0.5954,  3.7297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5776, -0.3027, -2.3218]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9433,  0.0750,  1.0721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3692,  1.1203,  1.3911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4885,  0.1252, -0.5830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2006, -0.4764, -2.7015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5656, -0.6993,  4.0396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1637,  0.2966, -0.3891]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4329,  0.1494,  0.4188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5993, -0.0900, -2.6334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0095,  0.8155,  1.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5589,  1.7030, -0.0601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4465, -0.2264,  3.2627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6398,  0.0435,  3.0075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4329,  0.1494,  0.4188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1548, -0.1380,  2.8944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5993, -0.0900, -2.6334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0095,  0.8155,  1.4970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5589,  1.7030, -0.0601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4465, -0.2264,  3.2627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6398,  0.0435,  3.0075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1842,  0.0284,  0.2722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7238,  1.5506, -1.0201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7040, -0.2630, -2.3975]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.2337, -0.3092,  3.1191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Run polarity score on the entire dataset\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "                text = row['Reviews']\n",
    "                rate = row['Rating']\n",
    "                vader_result = sia.polarity_scores(text)\n",
    "                #rename\n",
    "                vader_result_rename = {}\n",
    "                for key, value in vader_result.items():\n",
    "                        vader_result_rename[f\"vader_{key}\"] = value\n",
    "                        \n",
    "                roberta_result = polarity_scores_roberta(text)\n",
    "                both= {**vader_result_rename,  **roberta_result}\n",
    "                results[rate] = both\n",
    "        except RuntimeError:\n",
    "                print(f\"Broke for the rating: {rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.964155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.087215</td>\n",
       "      <td>0.847931</td>\n",
       "      <td>0.064854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.1815</td>\n",
       "      <td>0.945584</td>\n",
       "      <td>0.048659</td>\n",
       "      <td>0.005757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.1611</td>\n",
       "      <td>0.965136</td>\n",
       "      <td>0.031254</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>0.490710</td>\n",
       "      <td>0.341235</td>\n",
       "      <td>0.168055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  vader_neg  vader_neu  vader_pos  vader_compound  roberta_neg  \\\n",
       "0      5      0.005      0.753      0.242          0.9943     0.004565   \n",
       "1      4      0.000      0.721      0.279          0.4767     0.087215   \n",
       "2      1      0.196      0.661      0.143         -0.1815     0.945584   \n",
       "3      2      0.186      0.714      0.100         -0.1611     0.965136   \n",
       "4      3      0.056      0.819      0.125          0.7553     0.490710   \n",
       "\n",
       "   roberta_neu  roberta_pos  \n",
       "0     0.031279     0.964155  \n",
       "1     0.847931     0.064854  \n",
       "2     0.048659     0.005757  \n",
       "3     0.031254     0.003609  \n",
       "4     0.341235     0.168055  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
